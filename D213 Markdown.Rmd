---
title: "D213 Task 2"
author: "Nicholas Hurd"
date: "2024-03-03"
output: html_document
---

```{r}
# Load the data
imdb <- read.csv("C:/Users/nahur/Desktop/WGU/D213 Advanced Data Analytics/Task 2/Datasets/sentiment labelled sentences/imdb_labelled.txt", quote = "", header = F, sep = "\t")
amazon <- read.csv("C:/Users/nahur/Desktop/WGU/D213 Advanced Data Analytics/Task 2/Datasets/sentiment labelled sentences/amazon_cells_labelled.txt", quote = "", header = F, sep = "\t")
yelp <- read.csv("C:/Users/nahur/Desktop/WGU/D213 Advanced Data Analytics/Task 2/Datasets/sentiment labelled sentences/yelp_labelled.txt", quote = "", header = F, sep = "\t")  
```

``` {r warning=FALSE, message = FALSE}
# Installation of packages and libraries 
library(magrittr)
library(dplyr)
library(tidytext)
library(neuralnet)
library(keras)
library(tensorflow)
library(tidyverse)
library(reticulate)
library(ggplot2)
library(purrr)
```

``` {r}
imdb <- imdb %>% 
  rename(review = V1, score = V2)

amazon <- amazon %>% 
  rename(review = V1, score = V2)

yelp <- yelp %>% 
  rename(review = V1, score = V2)

combined_data <- rbind(yelp, amazon)
combined_data <- rbind(combined_data, imdb)
```

```{r}
summary(combined_data)
sum(duplicated(combined_data)) # 17 duplicates 
combined_data <- combined_data[!duplicated(combined_data, fromLast = TRUE), ] # Keep first occurrence 
sum(duplicated(combined_data))

colSums(is.na(combined_data))
deltat(combined_data) 
str(combined_data)
head(combined_data)
is.null(combined_data)
```

``` {r}
# Character/word count
combined_data$review %>%
  strsplit(" ") %>%
  sapply(length) %>%
  summary()
```

```{r, results = 'hide'}
# Remove non-alphanumeric characters from strings
strsplit(combined_data$review, "[^a-zA-Z0-9]+")
```

``` {r}
# Adress Emojis 
row_with_emojis <- grepl("[\U{1F600}-\U{1F64F}\U{2702}-\U{27B0}\U{1F680}-\U{1F6C0}\U{1F170}-\U{1F251}\U{1F004}\U{1F0CF}\U{1F004}\U{1F004}]", combined_data$review, perl = TRUE)
sum(row_with_emojis)
```

``` {r}
# Vocab size
tidy_combined_data <- combined_data %>%
  unnest_tokens(word, review)

vocab_size <- tidy_combined_data %>% 
  count(word) %>%
  nrow()
cat("Vocabulary Size:", vocab_size)
```

``` {r}
# Count number of words 
combined_data$review %>% 
  strsplit(" ") %>% 
  sapply(length) %>%
  summary()
```

``` {r}
# Sentence Lengths
sent_length <- sapply(strsplit(combined_data$review, "\\s+"), length)
summary(sent_length)
```

``` {r}
# B1c - Word embedding length & B1d - Statistical justification for chosen max seq length 
max_seq_embed <- as.integer(round(sqrt(sqrt(vocab_size)), 0))
max_seq_length <- max(sent_length)
med_seq_length <- median(sent_length)
min_seq_length <- min(sent_length)
cat("Estimated Embedding Length:", max_seq_embed, 
    "\nMaximum Sequence Length:", max_seq_length, 
    "\nMedian Sequence Length:", med_seq_length, 
    "\nMinimum Sequence Length:", min_seq_length)
```

``` {r}
# B2 - Tokenization 
# NEXT 2 LINES WERE EXECUTED ABOVE IN B1b
tidy_combined_data <- combined_data %>%
  unnest_tokens(word, review)
head(tidy_combined_data)
```

``` {r}
# B3 - Padding Process 
tokenizer <- text_tokenizer()
tokenizer$fit_on_texts(tidy_combined_data$word)
word_indicies <- tokenizer$texts_to_sequences(tidy_combined_data$word)

padded_seq <- pad_sequences(word_indicies, maxlen = max_seq_length, padding = "post", truncating = "post")
```

``` {r}
# B3b
single_pad_seq_index <- 8
single_pad_seq <- padded_seq[single_pad_seq_index,]
single_pad_seq
```

``` {r}
# B4 
num_classes <- 2

model <- keras_model_sequential() %>%
  layer_dense(units = num_classes, activation = "softmax")
```

``` {r}
# B5
# Split the data 

set.seed(42)

# Define proportions for train, validation, and test sets
train_split <- 0.6
validation_split <- 0.2
test_split <- 0.2

# Determine the number of samples for each split
num_samples <- nrow(tidy_combined_data)
num_train <- round(train_split * num_samples)
num_validation <- round(validation_split * num_samples)
num_test <- num_samples - num_train - num_validation

# Randomly shuffle the data
shuffled_data <- tidy_combined_data[sample(nrow(tidy_combined_data)), ]

# Split the data into train, validation, and test sets
train_data <- shuffled_data[1:num_train, ]
validation_data <- shuffled_data[(num_train + 1):(num_train + num_validation), ]
test_data <- shuffled_data[(num_train + num_validation + 1):(num_train + num_validation + num_test), ]
```

# Part C








